


function [] = featureSelection()
    numSamples = 1000;
    numRelevantFeatures = 3;
    numTotalFeatures = 7;
    mirrorRelevant =0;
    mirrorNonRelevant= 0;

    trainingData = generateData(numSamples,numRelevantFeatures,numTotalFeatures,mirrorRelevant,mirrorNonRelevant);
    
    testData = generateData(numSamples,numRelevantFeatures,numTotalFeatures,mirrorRelevant,mirrorNonRelevant);
    
    [bruteTraingingError, bruteTestError] = bruteForce(trainingData,testData);
    [lassoTrainingError,lassoTestError,lassoLambdas] = lassoRegression(trainingData,testData);
    
    [ridgeTrainingError,ridgeTestError, ridgeLambdas] = ridgeRegression(trainingData,testData,lassoLambdas);
    
    x = 1:length(lassoTestError);
    
    plot(x,lassoTestError,'.-b');
    hold on, plot(x,ridgeTestError, '.-r');
    
    for i=1:length(bruteTestError)
        %i.e. for subsets of size 3, x=3
        x = i*ones(length(bruteTestError{i}),1);
    end%plot brute errror
    
    
    
end%main  - featureSelection




function [trainingErrors,testErrors] = bruteForce(trainingData,testData)

    numFeatures = size(trainingData,2)-1;%-1 because the class is the last column
    featureIndices = [1:numFeatures];
    
    
    
    %the ith cell will hold all combinations of indices for a subsrt of
    %size i features. i.e. cell 2 = [1 2;1 3;1 4...2 3;2 4...]
    featureIndicesToUse = {};
    
    for i=1:numFeatures
        featureIndicesToUse{i} = combnk(featureIndices,i);
    end
    
    %each cell will be a vector of errors, one for each subset of that size
    testErrors = cell(1,numFeatures);
    trainingErrors = cell(1,numFeatures);
        
    %i goes from 1 to numFeatures
    for i=1:length(featureIndicesToUse)
        subsets =  featureIndicesToUse{i};
        curTestError= -ones(length(subsets),1);
        curTrainingError= -ones(length(subsets),1); 
        for j=1:length(subsets)
             curTestError(j) =doLinearRegression(trainingData,testData,subsets(j));
             curTrainingError(j) =doLinearRegression(trainingData,trainingData,subsets(j));
        end
        testErrors{i} = curTestError;
        trainingErrors{i} = curTrainingError;
    end%for i    
end%end brute force


function [percentWrong] = doLinearRegression(trainingData,testData,featureIndicesToUse)

    %last column in trainingData is the class(y)
    lastCol = size(trainingData,2);
    if(lastCol ~= size(testData,2))
        disp('Error in linear regression data set sizes');
        percentWrong = -1;
        return
    end
    
    
    
    %fitlm does a bias, doesn't need(or want) a column of all 1's
    w = fitlm(trainingData(:,featureIndicesToUse),trainingData(:,lastCol));%.Coefficients{:,1};
    
    w = w.Coefficients{:,1};
    %dont work cause bias
    
    hypothesis = [ones(size(testData,1),1) testData(:,featureIndicesToUse)] *w ;
    
    error = (hypothesis - testData(:,lastCol)).^2;
    
    for i=1:length(hypothesis)
        if(hypothesis(i) < 0)
            hypothesis(i) = -1;
        else
            hypothesis(i) =1;
        end
    end%end for
    
    numWrong = sum(abs(hypothesis-testData(:,lastCol)))/2;
    percentWrong = numWrong/size(testData,1);

end%doLinearRegression


function [trainingErrors,testErrors,lambdas] = lassoRegression(trainingData,testData)

    numFeatures = size(trainingData,2)-1;%-1 because the class is the last column
    
    %b has coeffieciets for each value of lambda used
    [B STATS] = lasso(trainingData(:,1:end-1),trainingData(:,end),'NumLambda',1000,'LambdaRatio',.0001);
    
    %number of featrues used for each lambda. 
    %numFeatures...numFeatures-1......0
    featuresPerLambda = STATS.DF; 
    lambdas = STATS.Lambda; %all the lambdas used
    
    
    %each cell is a subset of numFeaturesPerLamba
    %cell 1 has all the 1's from numFeaturesLambda, and so on
    
    
    %how many trials we've looked at so far
    soFar = 0;
    %use the largest lambda for each subset size (subset of features)
    trialsToUse = getTrialsToUse(featuresPerLambda,numFeatures);
    %so for each possible number of features to use,  pick one
    %set of coefficients coresponding to the largest lambda for that number
    
    
   %get errors
   testErrors = -ones(numFeatures,1);
   trainingErrors = -ones(numFeatures,1);
   for i=1:numFeatures
       if(trialsToUse(i) <1)
           continue;
       end;
       testErrors(i) = getError(B(1:numFeatures,trialsToUse(i)),testData);
       trainingErrors(i) = getError(B(1:numFeatures,trialsToUse(i)),trainingData);
   end
end%end doLassoRegression








function [trainingErrors,testErrors,lambdas] = ridgeRegression(trainingData,testData,lambdas)

    numFeatures = size(trainingData,2)-1;%-1 because the class is the last column
    B0 = zeros(numFeatures,length(lambdas));%one coeffincet vector for each lambda
    
    
    lambdas = lambdas*10000000;
    
    for i=1:length(lambdas)
        B0(:,i) = ridge(trainingData(:,end),trainingData(:,1:end-1),lambdas(i));
    end
    B0(abs(B0) <.0005) = 0;
    
    
    Bcount = B0;
    Bcount(Bcount~=0) = 1;
    
    featuresPerLambda = -ones(1,size(Bcount,2));
    for i=1:size(Bcount,2)
       featuresPerLambda(i) =  sum(Bcount(:,i));
    end
       
       
       
    %same as LASSO   
    trialsToUse = getTrialsToUse(featuresPerLambda,numFeatures);
    
    %get errors
   testErrors = -ones(numFeatures,1);
   trainingErrors = -ones(numFeatures,1);
   for i=1:numFeatures
       if(trialsToUse(i) <1)
           continue;
       end;
       testErrors(i) = getError(B0(1:numFeatures,trialsToUse(i)),testData);
       trainingErrors(i) = getError(B0(1:numFeatures,trialsToUse(i)),trainingData);
   end
    

end%ridgeRegression






function [trialsToUse] = getTrialsToUse(featuresPerLambda,numFeatures)

    %how many trials we've looked at so far
    soFar = 0;
    %use the largest lambda for each subset size (subset of features)
    trialsToUse = zeros(numFeatures,1);
    for i=numFeatures:-1:1
               %gets number of lambdas used per subset size
        trialsDone = length(featuresPerLambda(featuresPerLambda == i));
        if(trialsDone >0)
            trialsToUse(i) =  trialsDone+ soFar;
            soFar = trialsDone + soFar;
        else
            trialsToUse(i) = -1;
        end
    end
    %so for each possible number of features to use,  pick one
    %set of coefficients coresponding to the largest lambda for that number
    
end%getTrialsToUse




function [data] =  generateData(numSamples,numRelevantFeatures,numTotalFeatures,mirrorRelevant,mirrorNonRelevant)

    %generate the relevant features for class 1
               %mean -4, standard deviation 2
    xPos =  -4 + 2.*randn(numSamples,numRelevantFeatures) + (2 + 4.*randn(numSamples,numRelevantFeatures));
    xNeg =  4 + 2*randn(numSamples,numRelevantFeatures) +  (-2 + 4*randn(numSamples,numRelevantFeatures));
    
    %generate the non relevant features, just random numbers
    nonRel = -6 + (6--6).*rand(numSamples*2,numTotalFeatures-numRelevantFeatures);
    
    %put half in postive class, half in negative class
    xPos = [xPos nonRel(1:numSamples,:)];
    xNeg = [xNeg nonRel(numSamples+1:size(nonRel,1),:)];
    
    if(mirrorRelevant)
        if(numTotalFeatures > numRelevantFeatures)
            %make last feature(non-relevant) = -first feature(relevant)
            xPos(:,size(xPos,2)) = -xPos(:,1);
        else
            disp('no non relevant feature to make a mirror');
        end
    end%if mirror relevant

    if(mirrorRelevant)
        if(numTotalFeatures > (numRelevantFeatures+1))
            %make last feature = -next to last feature(both are not relevant
            xPos(:,size(xPos,2)) = -xPos(:,size(xPos,2)-1);
        else
            disp('not enough non relevant feature to make a mirror');
        end
    end%if mirror relevant
    
    %add classification
    xPos = [xPos ones(size(xPos,1),1)];
    xNeg = [xNeg -ones(size(xNeg,1),1)];
    
    data = [xPos;xNeg];
end%generate data




function [percentError] = getError(hypothesisCoeff,testData)

    hypothesis = testData(:,1:end-1) *hypothesisCoeff ;
    
    for i=1:length(hypothesis)
        if(hypothesis(i) < 0)
            hypothesis(i) = -1;
        else
            hypothesis(i) =1;
        end
    end%end for
    
    numWrong = sum(abs(hypothesis-testData(:,end)))/2;
    percentError = numWrong/size(testData,1);
end